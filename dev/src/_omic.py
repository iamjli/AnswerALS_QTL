#!/usr/bin/env python3
import pandas as pd
import numpy as np
import scipy.stats as stats
import warnings

from pathlib import Path

from . import BASE_DIR, RESULTS_PATHS, CHROMS, logger
from .bed import Regions


class Omic: 

	def __init__(self, counts, regions, feature, lengths, sample_frac_threshold=0.2, count_threshold=6, tpm_threshold=0.1): 
		"""
		Accept either raw counts, or phenotype file.
		 - for how counts were generated see sections 2.2 and 2.3: http://localhost:8894/notebooks/tensorqtl_runs/210321_prepare_tensorqtl.ipynb
		"""

		assert (feature == "gene") or (feature == "peak")
		self.counts  = counts.copy()
		self.regions = regions.copy()
		self.feature = feature

		# prepare regions and use for indexing
		if not self.regions.iloc[0,0].startswith("chr"): 
			self.regions.iloc[:,0] = "chr" + self.regions.iloc[:,0]
		self.regions.iloc[:,1:3] = self.regions.iloc[:,1:3].astype(int)

		if self.feature == "gene": 
			self.regions.columns = ["chrom", "start", "end", "strand"]
			self.regions.index.name = "gene_id"
		else: 
			self.regions.columns = ["chrom", "start", "end"]
			self.regions.index = self.regions["chrom"] + ":" + self.regions["start"].astype(str) + "-" + self.regions["end"].astype(str)
			self.regions.index.name = "peak_id"

		self.regions = Regions(self.regions)
		self.sample_names = self.counts.columns

		# compute lengths for TPM
		self.lengths = lengths.copy().rename("lengths")
		# if lengths is None: 
		# 	if self.feature == "gene":
		# 		logger.write("Warning: lengths should be specified for RNA.")
		# 	self.lengths = (self.regions["end"] - self.regions["start"]).astype(int).rename("lengths")
		# else: 
		# 	self.lengths = lengths.copy().rename("lengths")
		
		# get everything onto the same index
		self.counts.index  = self.regions.index
		self.lengths.index = self.regions.index

		# thresholding parameters
		self.sample_frac_threshold = sample_frac_threshold
		self.count_threshold = count_threshold
		self.tpm_threshold = tpm_threshold
		self._mask = None

		self._lib_norm_factors = None
		self._tmm_norm_factors = None

		# normalized martices
		self._tpm = None
		self._tmm = None
		self._gtex = None

	@staticmethod
	def _get_paths(data_dir, prefix): 
		"""Get paths of files generated by this class."""
		data_dir = Path(data_dir)
		paths = {
			"counts": data_dir / "_{}.counts.txt.gz".format(prefix),  # includes counts matrix and some feature metadata
			"norm_factors": data_dir / "_{}.norm_factors.txt.gz".format(prefix), 
			"gtex": data_dir / "{}_gtex.bed.gz".format(prefix)
		}
		return paths

	@classmethod
	def load(cls, data_dir, prefix, feature):
		"""Loads previously saved `Omic` objects."""
		paths = Omic._get_paths(data_dir, prefix)

		counts_df = pd.read_csv(paths["counts"], sep="\t", index_col=0, compression="gzip", low_memory=False)

		if feature == "gene": 
			assert counts_df.index.name == "gene_id"
			raw_counts = counts_df.iloc[:,5:]
			omic_obj = cls(
				counts  = raw_counts,
				regions = counts_df[["chrom", "start", "end", "strand"]],
				feature = feature,
				lengths = counts_df["lengths"]
			)
		if feature == "peak":
			assert counts_df.index.name == "peak_id"
			raw_counts = counts_df.iloc[:,4:]
			omic_obj = cls(
				counts  = raw_counts,
				regions = counts_df[["chrom", "start", "end"]],
				feature = feature,
				lengths = counts_df["lengths"]
			)

		# Load precomputed norm factors
		omic_obj.set_norm_factors(paths["norm_factors"])
		# Load precomputed gtex counts
		omic_obj._gtex = pd.read_csv(paths["gtex"], sep="\t", index_col=3, compression="gzip", low_memory=False).reindex(columns=raw_counts.columns)
		return omic_obj

	def set_norm_factors(self, path): 
		"""Set normalization factors from path"""
		norm_factors = pd.read_csv(path, sep="\t", index_col=0, compression="gzip", low_memory=False)
		self._lib_norm_factors = norm_factors["lib_norm_factors"]
		self._tmm_norm_factors = norm_factors["tmm_norm_factors"]

	@classmethod
	def load_rna(cls, data_dir, prefix): 
		logger.write("Loading saved RNA counts from: {}".format(dump_path.relative_to(BASE_DIR)))
		return cls.load(data_dir, prefix, feature="gene")

	@classmethod
	def load_atac(cls, data_dir, prefix): 
		logger.write("Loading saved ATAC counts from: {}".format(dump_path.relative_to(BASE_DIR)))
		return cls.load(data_dir, prefix, feature="peak")

	@property
	def mask(self):
		n_threshold = self.counts.shape[1] * self.sample_frac_threshold

		if self._mask is None: 
			if self.feature == "gene": 
				self._mask = (
					((self.counts >= self.count_threshold).sum(axis=1) >= n_threshold) & 
					((self.tpm >= self.tpm_threshold).sum(axis=1) >= n_threshold) & 
					(self.regions["chrom"].isin(CHROMS))
				)
			else: 
				self._mask = (
					((self.counts >= self.count_threshold).sum(axis=1) >= n_threshold) &
					((self.tpm >= self.tpm_threshold).sum(axis=1) >= n_threshold)
				)
		return self._mask

	# NORMALIZATION FACTORS
	@property
	def tmm_norm_factors(self):
		"""TMM normalization as implemented in edgeR."""
		if self._tmm_norm_factors is None: 
			logger.write("Computing TMM norm factors...")
			_tmm_norm_factors = edgeR_calcNormFactors(self.counts)
			self._tmm_norm_factors = pd.Series(data=_tmm_norm_factors, index=self.sample_names)
		return self._tmm_norm_factors

	@property
	def lib_norm_factors(self):
		"""Total number of million reads."""
		if self._lib_norm_factors is None: 
			if self.feature == "gene": 
				self._lib_norm_factors = self.counts.sum(axis=0) / 1e6
			elif self.feature == "peak": 
				# ATAC data has already been normalized by library size. This changed on 210331. 
				# self._lib_norm_factors = pd.Series(data=1, index=self.sample_names)  
				self._lib_norm_factors = self.counts.sum(axis=0) / 1e6
		return self._lib_norm_factors

	@property
	def edgeR_norm_factors(self):
		return self.lib_norm_factors * self.tmm_norm_factors

	# COUNT MATRICES
	@property
	def tpm(self):
		if self._tpm is None: 
			rpk = self.counts.div(self.lengths / 1000, axis=0)
			self._tpm = rpk.div(rpk.sum() / 1e6, axis=1)
		return self._tpm
	
	@property
	def tmm(self):
		"""edgeR normalization: normalize by library size (counts per million) and TMM factors."""
		if self._tmm is None: 
			norm_factors = self.lib_norm_factors * self.tmm_norm_factors
			self._tmm = self.counts / norm_factors
		return self._tmm

	@property
	def gtex(self):
		if self._gtex is None: 
			self._gtex = inverse_normal_transform(self.tmm[self.mask])
		return self._gtex
	
	# I/O
	def write_tensorqtl_phenotypes(self, output_dir, prefix): 

		# Format phenotype files for tensorqtl
		phenotypes_df = pd.concat([self.regions, self.gtex], axis=1, join="inner")
		phenotypes_df.index.name = "gene_id"
		phenotypes_df = phenotypes_df.reset_index().rename(columns={"chrom": "#chr"})
		phenotypes_df = phenotypes_df[["#chr", "start", "end", "gene_id"] + list(phenotypes_df.columns[4:])]
		phenotypes_df["#chr"] = phenotypes_df["#chr"].str[3:] # remove chromosome prefix

		# Write phenotype files for tensorqtl
		output_path = Path(output_dir) / "{}_gtex.bed.gz".format(prefix)
		phenotypes_df.to_csv(output_path, sep="\t", index=False, compression="gzip")

		# Write per-chromosome phenotype files for parallel processing
		for chrom in phenotypes_df["#chr"].unique(): 
			phenotypes_by_chrom_df = phenotypes_df[phenotypes_df["#chr"] == chrom]
			output_path = Path(output_dir) / "{}_gtex.chr{}.bed.gz".format(prefix, chrom)
			phenotypes_by_chrom_df.to_csv(output_path, sep="\t", index=False, compression="gzip")

		# Format for PEER correction. Remove strand column
		output_path = Path(output_dir) / "{}_gtex.for_PEER.bed.gz".format(prefix)
		phenotypes_df.drop(columns=["strand"], errors="ignore").to_csv(output_path, sep="\t", index=False, compression="gzip")

	def dump(self, output_dir, prefix): 
		"""Dump contents to load later."""
		paths = Omic._get_paths(output_dir, prefix)

		df = pd.concat([
			self.regions, 
			self.lengths.rename("lengths"), 
			self.counts
		], axis=1)

		df.to_csv(paths["counts"], sep="\t", index=True, compression="gzip")
		logger.write("Dumped counts and metadata to {}".format(paths["counts"]))
		
		# write norm factors
		norm_factors = pd.DataFrame({
			"lib_norm_factors": self.lib_norm_factors, 
			"tmm_norm_factors": self.tmm_norm_factors
		})
		norm_factors.to_csv(paths["norm_factors"], sep="\t", index=True, compression="gzip")
		logger.write("Dumped norm factors to {}".format(paths["norm_factors"]))
		

######################################
## 			NORMALIZATIONS			##
######################################

def edgeR_calcNormFactors(counts, ref=None, logratio_trim=0.3, sum_trim=0.05, acutoff=-1e10, verbose=False):
	"""
	Calculate TMM (Trimmed Mean of M values) normalization.
	Reproduces edgeR::calcNormFactors.default
	Scaling factors for the library sizes that minimize the log-fold changes between the samples for most genes.
	Effective library size: TMM scaling factor * library size
	References:
	 [1] Robinson & Oshlack, 2010
	 [2] R functions:
		  edgeR::calcNormFactors.default
		  edgeR:::.calcFactorWeighted
		  edgeR:::.calcFactorQuantile
	"""

	# discard genes with all-zero counts
	Y = counts.values.copy()
	allzero = np.sum(Y>0,axis=1)==0
	if np.any(allzero):
		Y = Y[~allzero,:]

	# select reference sample
	if ref is None:  # reference sample index
		# f75 = np.percentile(Y/np.sum(Y,axis=0), 75, axis=0)
		f75 = np.array([np.percentile(x,75) for x in (Y/np.sum(Y,axis=0)).T])
		ref = np.argmin(np.abs(f75-np.mean(f75)))
		if verbose:
			logger.write('Reference sample index: '+str(ref))

	N = np.sum(Y, axis=0)  # total reads in each library

	# with np.errstate(divide='ignore'):
	with warnings.catch_warnings():
		warnings.simplefilter('ignore')
		logR = np.log2((Y/N).T / (Y[:,ref]/N[ref])).T  # log fold change; Mg in [1]
		absE = 0.5*(np.log2(Y/N).T + np.log2(Y[:,ref]/N[ref])).T  # average log relative expression; Ag in [1]
		v = (N-Y)/N/Y
		v = (v.T + v[:,ref]).T  # w in [1]

	ns = Y.shape[1]
	tmm = np.zeros(ns)
	for i in range(ns):
		fin = np.isfinite(logR[:,i]) & np.isfinite(absE[:,i]) & (absE[:,i] > acutoff)
		n = np.sum(fin)

		loL = np.floor(n*logratio_trim)+1
		hiL = n + 1 - loL
		loS = np.floor(n*sum_trim)+1
		hiS = n + 1 - loS
		rankR = stats.rankdata(logR[fin,i])
		rankE = stats.rankdata(absE[fin,i])
		keep = (rankR >= loL) & (rankR <= hiL) & (rankE >= loS) & (rankE <= hiS)
		# in [1], w erroneously defined as 1/v ?
		tmm[i] = 2**(np.nansum(logR[fin,i][keep]/v[fin,i][keep]) / np.nansum(1/v[fin,i][keep]))

	tmm = tmm / np.exp(np.mean(np.log(tmm)))
	return tmm

def inverse_normal_transform(M):
	"""
	Transform rows to a standard normal distribution
	"""
	# logger.write("Computing inverse normal transform...")
	if isinstance(M, pd.Series): 
		M_ = M.to_frame().T
	else: 
		M_ = M

	R = stats.mstats.rankdata(M_, axis=1)  # ties are averaged
	Q = stats.norm.ppf(R/(M_.shape[1]+1))

	if isinstance(M, pd.DataFrame):
		return pd.DataFrame(Q, index=M.index, columns=M.columns)
	elif isinstance(M, pd.Series): 
		return pd.Series(Q.squeeze(), index=M.index)
	else:
		Q = stats.norm.ppf(R/(M_.shape[1]+1))
	return Q

def normalize_quantiles(df):
	"""
	Quantile normalization to the average empirical distribution
	Note: replicates behavior of R function normalize.quantiles from library("preprocessCore")
	Reference:
	 [1] Bolstad et al., Bioinformatics 19(2), pp. 185-193, 2003
	Adapted from https://github.com/andrewdyates/quantile_normalize
	"""
	M = df.values.copy()

	Q = M.argsort(axis=0)
	m,n = M.shape

	# compute quantile vector
	quantiles = np.zeros(m)
	for i in range(n):
		quantiles += M[Q[:,i],i]
	quantiles = quantiles / n

	for i in range(n):
		# Get equivalence classes; unique values == 0
		dupes = np.zeros(m, dtype=np.int)
		for j in range(m-1):
			if M[Q[j,i],i]==M[Q[j+1,i],i]:
				dupes[j+1] = dupes[j]+1

		# Replace column with quantile ranks
		M[Q[:,i],i] = quantiles

		# Average together equivalence classes
		j = m-1
		while j >= 0:
			if dupes[j] == 0:
				j -= 1
			else:
				idxs = Q[j-dupes[j]:j+1,i]
				M[idxs,i] = np.median(M[idxs,i])
				j -= 1 + dupes[j]
		assert j == -1

	return pd.DataFrame(M, index=df.index, columns=df.columns)